# vLLM Server Configuration
# This file contains environment variables for the vLLM service

# Model configuration
VLLM_MODEL="${VLLM_MODEL:-openai/gpt-oss-20b}"
MODEL_PATH="${MODEL_PATH:-}"

# Server settings
HOST="${HOST:-0.0.0.0}"
PORT="${PORT:-8000}"

# GPU memory management
GPU_MEMORY_UTILIZATION="${GPU_MEMORY_UTILIZATION:-0.5}" # 48 GB -> 24 GB
ASYNC_SCHEDULING="${ASYNC_SCHEDULING:-true}"

# Performance settings
TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-1}"
MAX_MODEL_LEN="${MAX_MODEL_LEN:-2048}"

# Additional vLLM parameters
DISABLE_LOG_REQUESTS="${DISABLE_LOG_REQUESTS:-false}"
MAX_NUM_SEQS="${MAX_NUM_SEQS:-16}"
